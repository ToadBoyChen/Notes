{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# www.mozilla.org\n",
    "\n",
    "## Details\n",
    "\n",
    "1. Core Site\n",
    "1. Mozilla Marketing Website aka Bedrock.\n",
    "1. In scope\n",
    "1. Severity - Critical\n",
    "1. Please use our staging instance, www.allizom.org, for testing to avoid site disruption.\n",
    "\n",
    "## Investigation\n",
    "\n",
    "1. Configure Burp Suite\n",
    "1. Spider the website\n",
    "    1. To spider we will use `scrapy`, but we must also bypass the `robots.txt` file. to do this goto the scrapy settings file and `ROBOTSTXT_OBEY = False  # Set to True to obey robots.txt rules`.\n",
    "    1. You should use a VPN to avoid getting banned from the website.\n",
    "    1. After this is set up, commit the following steps.\n",
    "        1. `scrapy startproject myproject`\n",
    "        1. `cd myproject`\n",
    "        1. `scrapy genspider myspider example.com`\n",
    "        1. `scrapy crawl myspider -o output.json`\n",
    "        1. Now simply open the file and analyse!\n",
    "\n",
    "\n",
    "When using scrapy, definitely have a list of sensitive keywords.\n",
    "\n",
    "```cmd\n",
    "sensitive_keywords = ['admin', 'dashboard', 'config', 'login', 'settings', 'account', 'user', 'control', 'management']\n",
    "```\n",
    "\n",
    "Consider changing your `spider.py` to:\n",
    "\n",
    "```cmd\n",
    "import logging\n",
    "import scrapy\n",
    "\n",
    "class SpiderSpider(scrapy.Spider):\n",
    "    name = \"spider\"\n",
    "    allowed_domains = [\"www.allizom.org\"]\n",
    "    start_urls = [\"https://www.allizom.org/\"]\n",
    "\n",
    "    # Use a set to store visited links\n",
    "    visited_links = set()\n",
    "\n",
    "    # Limit crawl depth to avoid going too deep\n",
    "    custom_settings = {\n",
    "        'DEPTH_LIMIT': 3,\n",
    "        'DOWNLOAD_DELAY': 2,  # Adjust to avoid hitting rate limits\n",
    "    }\n",
    "\n",
    "    def parse(self, response):\n",
    "        links = response.css('a::attr(href)').getall()\n",
    "        sensitive_keywords = ['admin', 'dashboard', 'config', 'login', 'settings', 'account', 'user', 'control', 'management']\n",
    "\n",
    "        for link in links:\n",
    "            # Normalize the link to avoid duplicates caused by differences in slashes or trailing '/'.\n",
    "            link = response.urljoin(link)  # This makes sure the URL is absolute.\n",
    "\n",
    "            # Check if link has already been visited\n",
    "            if link not in self.visited_links:\n",
    "                # Mark this link as visited\n",
    "                self.visited_links.add(link)\n",
    "\n",
    "                # Follow sensitive links\n",
    "                if any(keyword in link for keyword in sensitive_keywords):\n",
    "                    logging.info(f\"Found potentially sensitive page: {link}\")\n",
    "                    yield {'sensitive_page': link}\n",
    "\n",
    "                    yield response.follow(link, self.parse)\n",
    "\n",
    "                # Follow only internal links and avoid unnecessary file types\n",
    "                elif link.startswith('https://www.allizom.org') and not link.endswith(('.jpg', '.png', '.gif', '.pdf')):\n",
    "                    yield response.follow(link, self.parse)\n",
    "\n",
    "```\n",
    "\n",
    "though for intensive deep scans you could use:\n",
    "\n",
    "```cmd\n",
    "import logging\n",
    "import scrapy\n",
    "\n",
    "class SpiderSpider(scrapy.Spider):\n",
    "    name = \"spider\"\n",
    "    allowed_domains = [\"www.allizom.org\"]\n",
    "    start_urls = [\"https://www.allizom.org/en-GB/?v=1\"]\n",
    "\n",
    "    def parse(self, response):\n",
    "        logging.info(f\"Visited: {response.url}\")\n",
    "\n",
    "        # Extract titles\n",
    "        titles = response.css('h1::text').getall()\n",
    "        logging.info(f\"Found titles: {titles}\")\n",
    "        for title in titles:\n",
    "            yield {'title': title}\n",
    "\n",
    "        # Extract additional headings\n",
    "        h2_headings = response.css('h2::text').getall()\n",
    "        logging.info(f\"Found h2 headings: {h2_headings}\")\n",
    "        for h2 in h2_headings:\n",
    "            yield {'h2': h2}\n",
    "\n",
    "        # Extract all paragraph texts\n",
    "        paragraphs = response.css('p::text').getall()\n",
    "        logging.info(f\"Found paragraphs: {paragraphs}\")\n",
    "        for paragraph in paragraphs:\n",
    "            yield {'paragraph': paragraph}\n",
    "\n",
    "        # Extract all links and their URLs\n",
    "        links = response.css('a::text').getall()\n",
    "        link_urls = response.css('a::attr(href)').getall()\n",
    "        logging.info(f\"Found links: {links} with URLs: {link_urls}\")\n",
    "        for link, url in zip(links, link_urls):\n",
    "            yield {'link_text': link, 'link_url': url}\n",
    "\n",
    "        # Extract images and their sources\n",
    "        image_sources = response.css('img::attr(src)').getall()\n",
    "        logging.info(f\"Found image sources: {image_sources}\")\n",
    "        for src in image_sources:\n",
    "            yield {'image_src': src}\n",
    "\n",
    "        # Follow links to other pages if needed\n",
    "        for next_page in response.css('a::attr(href)').getall():\n",
    "            yield response.follow(next_page, self.parse)\n",
    "\n",
    "```\n",
    "\n",
    "## Extended Spiders\n",
    "\n",
    "### target Forms and Input Fields\n",
    "\n",
    "``` cmd\n",
    "def parse(self, response):\n",
    "    logging.info(f\"Visited: {response.url}\")\n",
    "\n",
    "    # Extract forms and input fields\n",
    "    forms = response.css('form')\n",
    "    for form in forms:\n",
    "        action = form.css('::attr(action)').get()\n",
    "        method = form.css('::attr(method)').get(default='GET')\n",
    "        inputs = form.css('input::attr(name)').getall()\n",
    "        logging.info(f\"Found form: {action}, Method: {method}, Inputs: {inputs}\")\n",
    "        \n",
    "        if action and ('login' in action or 'search' in action or 'submit' in action):\n",
    "            yield {\n",
    "                'form_action': action,\n",
    "                'form_method': method,\n",
    "                'input_fields': inputs\n",
    "            }\n",
    "```\n",
    "\n",
    "### target Potential Injection Points\n",
    "\n",
    "``` cmd\n",
    "def parse(self, response):\n",
    "    links = response.css('a::attr(href)').getall()\n",
    "    for link in links:\n",
    "        if '?' in link:  # Check if URL contains query parameters\n",
    "            yield {'potential_injection_point': link}\n",
    "\n",
    "        # Follow links to deepen your crawl\n",
    "        yield response.follow(link, self.parse)\n",
    "```\n",
    "\n",
    "### target XSS and Client-Side Issues\n",
    "\n",
    "``` cmd\n",
    "def parse(self, response):\n",
    "    scripts = response.css('script::attr(src)').getall()\n",
    "    for script in scripts:\n",
    "        if script.endswith('.js'):\n",
    "            logging.info(f\"Found JavaScript file: {script}\")\n",
    "            yield {'javascript_file': script}\n",
    "\n",
    "        # Follow JavaScript file links to scrape them as well\n",
    "        yield response.follow(script, self.parse)\n",
    "```\n",
    "\n",
    "### target Potential Misconfigurations\n",
    "\n",
    "``` cmd\n",
    "def parse(self, response):\n",
    "    hidden_inputs = response.css('input[type=\"hidden\"]::attr(value)').getall()\n",
    "    logging.info(f\"Found hidden inputs: {hidden_inputs}\")\n",
    "    for hidden in hidden_inputs:\n",
    "        yield {'hidden_input': hidden}\n",
    "\n",
    "```\n",
    "\n",
    "### target Administrative or Sensitive Pages\n",
    "\n",
    "``` cmd\n",
    "def parse(self, response):\n",
    "    links = response.css('a::attr(href)').getall()\n",
    "    for link in links:\n",
    "        if 'admin' in link or 'dashboard' in link or 'config' in link:\n",
    "            logging.info(f\"Found sensitive page: {link}\")\n",
    "            yield {'sensitive_page': link}\n",
    "\n",
    "        # Follow links to check for deeper vulnerabilities\n",
    "        yield response.follow(link, self.parse)\n",
    "```\n",
    "\n",
    "### target Session Management Issues\n",
    "\n",
    "``` cmd\n",
    "def parse(self, response):\n",
    "    cookies = response.headers.getlist('Set-Cookie')\n",
    "    for cookie in cookies:\n",
    "        if 'HttpOnly' not in cookie.decode() or 'Secure' not in cookie.decode():\n",
    "            yield {'insecure_cookie': cookie.decode()}\n",
    "\n",
    "```\n",
    "\n",
    "## URLs\n",
    "\n",
    "URLs of interest:\n",
    "\n",
    "### https://www.allizom.org/media/js/data.79f9e875d181.js\n",
    "\n",
    "\n",
    "This code appears to be a JavaScript module designed for tracking download events on a website, likely related to Mozilla's products. Here’s a breakdown of its main components and functionality:\n",
    "\n",
    "Strict Mode: The code is executed in strict mode, which enforces stricter parsing and error handling in JavaScript.\n",
    "\n",
    "Download URL Validation:\n",
    "\n",
    "The isValidDownloadURL function checks if a given URL matches any of the predefined patterns for valid download URLs, which include various domains associated with Mozilla, Apple, Google Play, and Microsoft Store.\n",
    "Event Object Creation:\n",
    "\n",
    "The getEventObject function constructs an event object for a download event based on parameters like product name, platform, method, and optional fields like release channel and download language.\n",
    "Event Extraction from URL:\n",
    "\n",
    "The getEventFromUrl function extracts relevant information from a URL (like product name and platform) and builds an event object if the URL is valid.\n",
    "Link Handling:\n",
    "\n",
    "The handleLink function is triggered when a link is clicked. It ensures that the event handler identifies the clicked link and sends the appropriate event for tracking.\n",
    "Event Sending:\n",
    "\n",
    "The sendEvent function pushes the event object into a global dataLayer array for analytics purposes and may also trigger events in other systems (like Mozilla's Glean).\n",
    "Event Listeners:\n",
    "\n",
    "The code adds click event listeners to elements with the class ga-product-download, allowing it to track when users click on specific download links.\n",
    "Experiment Tracking:\n",
    "\n",
    "The module also includes functionality for tracking experiments (likely A/B tests) via the experiment_view event.\n",
    "Overall, this code is focused on tracking downloads, sending relevant analytics data, and ensuring that download links are valid and properly categorized based on the product and platform. It integrates with Google's Tag Manager or a similar analytics framework to capture user interactions with download links.\n",
    "\n",
    "### Vulnerabilities\n",
    "\n",
    "Exploiting this code could involve several potential vulnerabilities or weaknesses, particularly if an attacker can manipulate or exploit how events are tracked or how URLs are processed. Here are some potential avenues for exploitation:\n",
    "\n",
    "Malicious URLs: If the validation of download URLs (isValidDownloadURL) is bypassed or not robust enough, an attacker could inject malicious URLs that trigger the tracking functions. This could lead to phishing attacks or malware downloads.\n",
    "\n",
    "Cross-Site Scripting (XSS): If user input is not properly sanitized before being processed (for instance, if an attacker can inject scripts via URLs or query parameters), it could lead to XSS vulnerabilities. This would allow attackers to execute arbitrary JavaScript in the context of a user’s session.\n",
    "\n",
    "Data Leakage: The code uses a global dataLayer to push event data. If sensitive information is inadvertently included in these events, an attacker could potentially capture this data, especially if it’s accessible in the browser.\n",
    "\n",
    "Manipulating Event Tracking: An attacker could manipulate the link handling or the event creation process to spoof download events or analytics data. This could distort analytics results and impact business decisions based on the data.\n",
    "\n",
    "Denial of Service (DoS): If an attacker could trigger a large number of events in a short period (e.g., through automated scripts), it might overload the analytics service or disrupt normal operations.\n",
    "\n",
    "Browser-Specific Exploits: If the code is only tested on specific browsers or platforms, attackers could exploit known vulnerabilities in those environments that the code relies on, especially if the code assumes certain APIs or behaviors.\n",
    "\n",
    "Social Engineering: If the tracking mechanisms create a false sense of security around certain download links, attackers could craft social engineering attacks to trick users into clicking those links, thinking they are safe.\n",
    "\n",
    "To mitigate these risks, it’s important to:\n",
    "\n",
    "Ensure thorough validation and sanitization of any user input.\n",
    "Implement Content Security Policies (CSP) to restrict the sources of executable scripts.\n",
    "Regularly audit the code and dependencies for vulnerabilities.\n",
    "Use secure coding practices and keep libraries up to date.\n",
    "\n",
    "### https://www.allizom.org/en-US/privacy/websites/cookie-settings/\n",
    "\n",
    "### Vulnerability\n",
    "\n",
    "Key Security Headers\n",
    "X-Frame-Options: DENY\n",
    "\n",
    "Prevents the page from being embedded in iframes, mitigating clickjacking attacks.\n",
    "Content-Security-Policy (CSP)\n",
    "\n",
    "This policy specifies which resources can be loaded by the browser. It includes directives for script-src, style-src, and others, helping to prevent XSS and data injection attacks. However, the use of 'unsafe-inline' and 'unsafe-eval' can weaken this policy and may allow some XSS attacks.\n",
    "Strict-Transport-Security (HSTS)\n",
    "\n",
    "Ensures that the browser only communicates with the server over HTTPS, protecting against man-in-the-middle attacks. The max-age of 31536000 seconds (1 year) indicates a strong enforcement period.\n",
    "X-Content-Type-Options: nosniff\n",
    "\n",
    "Prevents browsers from MIME-sniffing a response away from the declared content type, reducing the risk of certain types of attacks.\n",
    "Referrer-Policy: strict-origin-when-cross-origin\n",
    "\n",
    "Controls how much referrer information is passed when navigating to different origins, enhancing privacy.\n",
    "Cross-Origin-Opener-Policy: same-origin\n",
    "\n",
    "Helps mitigate side-channel attacks by controlling how cross-origin documents can interact with each other.\n",
    "Other Useful Information\n",
    "Cache-Control and Expires: The page sets a cache duration, which can be important for ensuring that users receive updated content and reducing the risk of serving stale or sensitive information.\n",
    "\n",
    "Content-Language: Indicates the language of the content (English, in this case), which can be useful for localization and accessibility considerations.\n",
    "\n",
    "X-Clacks-Overhead: A humorous header that honors Terry Pratchett, indicating a light-hearted approach but does not impact security.\n",
    "\n",
    "Potential Concerns\n",
    "CSP Weaknesses: The presence of 'unsafe-inline' and 'unsafe-eval' suggests that there might be risks for XSS if any user input is not properly sanitized. Testing for XSS vulnerabilities is recommended.\n",
    "Recommendations for Further Testing\n",
    "Test for XSS: Attempt to inject scripts to see if any XSS vulnerabilities exist due to the CSP configuration.\n",
    "Check Resource Loading: Analyze the effectiveness of the CSP by checking if it properly blocks unintended scripts or resources.\n",
    "Evaluate Cookie Security: Inspect any cookies set by the page to ensure they have appropriate flags (e.g., HttpOnly, Secure).\n",
    "Overall, the headers indicate a strong security posture, but there are areas to test further, particularly around CSP configurations and potential XSS vectors."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
